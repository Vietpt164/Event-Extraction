{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10324487,"sourceType":"datasetVersion","datasetId":6392416},{"sourceId":10340932,"sourceType":"datasetVersion","datasetId":6403388}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:20.354412Z","iopub.execute_input":"2024-12-31T09:23:20.354737Z","iopub.status.idle":"2024-12-31T09:23:20.650693Z","shell.execute_reply.started":"2024-12-31T09:23:20.354710Z","shell.execute_reply":"2024-12-31T09:23:20.650025Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ee-pretrained/event-classififcation-pretrained.pt\n/kaggle/input/ee-pretrained/trigger-detection-pretrained.pt\n/kaggle/input/ee-pretrain-biobert/trigger-detection-biobert.pt\n/kaggle/input/ee-pretrain-biobert/event-cls-biobert.pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom transformers import BertTokenizerFast, BertForTokenClassification, BertTokenizer, BertModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:20.651656Z","iopub.execute_input":"2024-12-31T09:23:20.652063Z","iopub.status.idle":"2024-12-31T09:23:25.551794Z","shell.execute_reply.started":"2024-12-31T09:23:20.652040Z","shell.execute_reply":"2024-12-31T09:23:25.551164Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def load_trigger_model(model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    \"\"\"\n    Tải tokenizer và mô hình từ đường dẫn đã lưu.\n    \"\"\"\n    tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n\n    model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels=2)\n\n    # Tải state_dict từ tệp .pt\n    state_dict = torch.load(model_path, map_location=device)\n    model.load_state_dict(state_dict)\n\n    model.to(device)\n    model.eval()\n    return tokenizer, model, device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:25.553050Z","iopub.execute_input":"2024-12-31T09:23:25.553462Z","iopub.status.idle":"2024-12-31T09:23:25.630136Z","shell.execute_reply.started":"2024-12-31T09:23:25.553439Z","shell.execute_reply":"2024-12-31T09:23:25.629177Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def trigger_predict(sentence, tokenizer, model, device, max_len=128):\n    \"\"\"\n    Dự đoán nhãn cho một câu đầu vào.\n    \"\"\"\n    words = sentence.split()\n    encoding = tokenizer(\n        words,\n        is_split_into_words=True,\n        return_offsets_mapping=False,\n        padding='max_length',\n        truncation=True,\n        max_length=max_len,\n        return_tensors='pt'\n    )\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n    word_ids = encoding.word_ids(batch_index=0)\n\n    labels = []\n    previous_word_idx = None\n    for idx, word_idx in enumerate(word_ids):\n        if word_idx is None:\n            continue\n        elif word_idx != previous_word_idx:\n            labels.append(preds[idx])\n            previous_word_idx = word_idx\n\n    return list(zip(words, labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:25.631687Z","iopub.execute_input":"2024-12-31T09:23:25.632031Z","iopub.status.idle":"2024-12-31T09:23:25.648488Z","shell.execute_reply.started":"2024-12-31T09:23:25.631997Z","shell.execute_reply":"2024-12-31T09:23:25.647744Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"label_to_index_model2 = {\n    \"Negative_regulation\": 0,\n    \"Gene_expression\": 1,\n    \"Regulation\": 2,\n    \"Transcription\": 3,\n    \"Positive_regulation\": 4,\n    \"Binding\": 5,\n    \"Localization\": 6,\n    \"Phosphorylation\": 7,\n    \"Protein_catabolism\": 8,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:25.649415Z","iopub.execute_input":"2024-12-31T09:23:25.649750Z","iopub.status.idle":"2024-12-31T09:23:25.665576Z","shell.execute_reply.started":"2024-12-31T09:23:25.649710Z","shell.execute_reply":"2024-12-31T09:23:25.664883Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    \"\"\"\n    BERT-based classifier for event classification.\n    \"\"\"\n\n    def __init__(self, num_classes):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n        self.dropout = nn.Dropout(0.3)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        dropout_output = self.dropout(pooled_output)\n        return self.classifier(dropout_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:25.666353Z","iopub.execute_input":"2024-12-31T09:23:25.666655Z","iopub.status.idle":"2024-12-31T09:23:25.678186Z","shell.execute_reply.started":"2024-12-31T09:23:25.666634Z","shell.execute_reply":"2024-12-31T09:23:25.677408Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_event_model(model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    \"\"\"\n    Load the Event Classification tokenizer and model from the saved path.\n    \"\"\"\n    # Check if the model file exists\n    # if not os.path.isfile(model_path):\n    #     raise FileNotFoundError(f\"Event Classification model file '{model_path}' does not exist.\")\n\n    # Load the tokenizer for Event Classification\n    tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n\n    # Initialize the Event Classification model with the appropriate number of classes\n    num_classes = len(label_to_index_model2)\n    model = BertClassifier(num_classes=num_classes)\n\n    # Load the state_dict from the .pt file\n    state_dict = torch.load(model_path, map_location=device)\n\n    # If the state_dict has 'module.' prefix (from DataParallel), remove it\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        if k.startswith('module.'):\n            k = k[len('module.'):]\n        new_state_dict[k] = v\n\n    # Load the state_dict into the model\n    model.load_state_dict(new_state_dict)\n    model.to(device)\n    model.eval()\n\n    return tokenizer, model, device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:25.679004Z","iopub.execute_input":"2024-12-31T09:23:25.679251Z","iopub.status.idle":"2024-12-31T09:23:25.691134Z","shell.execute_reply.started":"2024-12-31T09:23:25.679231Z","shell.execute_reply":"2024-12-31T09:23:25.690382Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def prepare_input(words, tokenizer, max_len=32, is_split_into_words=True):\n    \"\"\"\n    Prepare the input for the models by tokenizing the words.\n    :param words: List of words to tokenize.\n    :param tokenizer: Tokenizer to use.\n    :param max_len: Maximum length of the token sequence.\n    :param is_split_into_words: Whether the input is already split into words.\n    :return: input_ids and attention_mask tensors.\n    \"\"\"\n    encoding = tokenizer(\n        words,\n        is_split_into_words=is_split_into_words,\n        return_offsets_mapping=False,\n        padding='max_length',\n        truncation=True,\n        max_length=max_len,\n        return_tensors='pt'\n    )\n    input_ids = encoding['input_ids']\n    attention_mask = encoding['attention_mask']\n    return input_ids, attention_mask\n\n\nindex_to_label_model2 = {v: k for k, v in label_to_index_model2.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:25.692984Z","iopub.execute_input":"2024-12-31T09:23:25.693250Z","iopub.status.idle":"2024-12-31T09:23:25.707873Z","shell.execute_reply.started":"2024-12-31T09:23:25.693229Z","shell.execute_reply":"2024-12-31T09:23:25.707031Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def predict_event(word, tokenizer, model, device, max_len=32):\n    \"\"\"\n    Predict the event type for a single trigger word using the Event Classification model.\n    :param word: The trigger word to classify.\n    :param tokenizer: Tokenizer for Event Classification model.\n    :param model: Event Classification model.\n    :param device: Device to run the model on.\n    :param max_len: Maximum length for tokenization.\n    :return: Predicted event type label.\n    \"\"\"\n    model.eval()\n    # Tokenize the single word\n    input_ids, attention_mask = prepare_input([word], tokenizer, max_len)\n    input_ids = input_ids.to(device)\n    attention_mask = attention_mask.to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n\n    # Map the prediction to the corresponding label\n    predicted_label = index_to_label_model2.get(preds[0], \"Unknown\")\n    return predicted_label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:25.708992Z","iopub.execute_input":"2024-12-31T09:23:25.709238Z","iopub.status.idle":"2024-12-31T09:23:25.722721Z","shell.execute_reply.started":"2024-12-31T09:23:25.709206Z","shell.execute_reply":"2024-12-31T09:23:25.721985Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def main():\n    trigger_model_path = '/kaggle/input/ee-pretrain-biobert/trigger-detection-biobert.pt'\n    event_model_path = '/kaggle/input/ee-pretrain-biobert/event-cls-biobert.pt'\n    trigger_tokenizer, trigger_model, device = load_trigger_model(trigger_model_path)\n\n    sentence = \" Four of the 6 mAECA activated EC , manifested by increased IL-6 and vWF secretion .\"\n    predictions = trigger_predict(sentence, trigger_tokenizer, trigger_model, device)\n    print(\"Event triggers:\")\n    trigger_words = []\n    for word, label in predictions:\n        if label == 1:\n            print(f\"{word}: {'Trigger'}\")\n            trigger_words.append(word)\n    # Load the Event Classification model\n    try:\n        event_tokenizer, event_model, device_event = load_event_model(event_model_path)\n        print(\"Successfully loaded Event Classification tokenizer and model.\")\n    except FileNotFoundError as e:\n        print(e)\n        return\n\n    # If there are trigger words, perform Event Classification\n    if trigger_words:\n        print(\"\\nEvent Classification Results:\")\n        for trigger_word in trigger_words:\n            event_label = predict_event(trigger_word, event_tokenizer, event_model, device_event)\n            print(f\"{trigger_word}: {event_label}\")\n    else:\n        print(\"\\nNo Trigger Words detected in the sentence.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:29:02.994034Z","iopub.execute_input":"2024-12-31T09:29:02.994348Z","iopub.status.idle":"2024-12-31T09:29:03.000160Z","shell.execute_reply.started":"2024-12-31T09:29:02.994323Z","shell.execute_reply":"2024-12-31T09:29:02.999371Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:29:10.218356Z","iopub.execute_input":"2024-12-31T09:29:10.218657Z","iopub.status.idle":"2024-12-31T09:29:12.639504Z","shell.execute_reply.started":"2024-12-31T09:29:10.218633Z","shell.execute_reply":"2024-12-31T09:29:12.638515Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-3-db931a4f8dc2>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Event triggers:\nincreased: Trigger\nsecretion: Trigger\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-7-3d8246c43e69>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Successfully loaded Event Classification tokenizer and model.\n\nEvent Classification Results:\nincreased: Positive_regulation\nsecretion: Localization\n","output_type":"stream"}],"execution_count":21}]}