{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1713918,"sourceType":"datasetVersion","datasetId":1015825}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:24.681358Z","iopub.execute_input":"2025-01-01T20:51:24.681711Z","iopub.status.idle":"2025-01-01T20:51:24.966706Z","shell.execute_reply.started":"2025-01-01T20:51:24.681683Z","shell.execute_reply":"2025-01-01T20:51:24.965997Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/genia-biomedical-event-dataset/train_data.csv\n/kaggle/input/genia-biomedical-event-dataset/test_data.csv\n/kaggle/input/genia-biomedical-event-dataset/dev_data.csv\n/kaggle/input/genia-biomedical-event-dataset/GE11-LICENSE\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:24.967710Z","iopub.execute_input":"2025-01-01T20:51:24.967992Z","iopub.status.idle":"2025-01-01T20:51:31.152836Z","shell.execute_reply.started":"2025-01-01T20:51:24.967974Z","shell.execute_reply":"2025-01-01T20:51:31.152121Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load data\ndata = pd.read_csv('/kaggle/input/genia-biomedical-event-dataset/train_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:31.154575Z","iopub.execute_input":"2025-01-01T20:51:31.155010Z","iopub.status.idle":"2025-01-01T20:51:31.211474Z","shell.execute_reply.started":"2025-01-01T20:51:31.154987Z","shell.execute_reply":"2025-01-01T20:51:31.210687Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define constants\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 32\nEPOCHS = 10\nLEARNING_RATE = 2e-5\nMODEL_SAVE_PATH = \"bert_classification_model.pt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:31.212670Z","iopub.execute_input":"2025-01-01T20:51:31.212898Z","iopub.status.idle":"2025-01-01T20:51:31.292178Z","shell.execute_reply.started":"2025-01-01T20:51:31.212877Z","shell.execute_reply":"2025-01-01T20:51:31.291442Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Dataset class\nclass EventDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for tokenizing and managing event data.\n    \"\"\"\n    def __init__(self, words, labels, tokenizer, max_len):\n        self.words = words\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.words)\n\n    def __getitem__(self, index):\n        word = self.words[index]\n        label = self.labels[index]\n        encoding = self.tokenizer(\n            word,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:31.292901Z","iopub.execute_input":"2025-01-01T20:51:31.293100Z","iopub.status.idle":"2025-01-01T20:51:31.307599Z","shell.execute_reply.started":"2025-01-01T20:51:31.293083Z","shell.execute_reply":"2025-01-01T20:51:31.306792Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Prepare data\nall_words = []\nlabels = []\n\n# Generate label indices and map \"No Event\" to 0\nlabel_to_index = {\"No Event\": 0}\ncurrent_index = 1\nfor row in data.itertuples():\n    trigger_words = str(row.TriggerWord).split(';') if pd.notna(row.TriggerWord) else []\n    event_types = str(row.EventType).split(';') if pd.notna(row.EventType) else []\n    word_to_class = dict(zip(trigger_words, event_types))\n\n    seen_words = set()\n    for word in row.Sentence.split():\n        if word in seen_words:\n            continue\n        seen_words.add(word)\n        all_words.append(word)\n        if word in word_to_class:\n            event_class = word_to_class[word]\n            if event_class not in label_to_index:\n                label_to_index[event_class] = current_index\n                current_index += 1\n            labels.append(label_to_index[event_class])\n        else:\n            labels.append(label_to_index[\"No Event\"])\n\n# Filter out \"No Event\" samples from training and validation data\ntrain_words, val_words, train_labels, val_labels = train_test_split(\n    all_words, labels, test_size=0.2, random_state=42\n)\n\ntrain_filtered = [(w, l) for w, l in zip(train_words, train_labels) if l != label_to_index[\"No Event\"]]\nval_filtered = [(w, l) for w, l in zip(val_words, val_labels) if l != label_to_index[\"No Event\"]]\n\ntrain_words, train_labels = zip(*train_filtered)\nval_words, val_labels = zip(*val_filtered)\n\n# Reset labels to be consecutive integers\nunique_labels = sorted(set(train_labels))\nlabel_remap = {old_label: new_index for new_index, old_label in enumerate(unique_labels)}\ntrain_labels = [label_remap[label] for label in train_labels]\nval_labels = [label_remap[label] for label in val_labels]\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n\n# Dataset and DataLoader\ntrain_dataset = EventDataset(list(train_words), list(train_labels), tokenizer, max_len=32)\nval_dataset = EventDataset(list(val_words), list(val_labels), tokenizer, max_len=32)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:31.308206Z","iopub.execute_input":"2025-01-01T20:51:31.308447Z","iopub.status.idle":"2025-01-01T20:51:32.092005Z","shell.execute_reply.started":"2025-01-01T20:51:31.308416Z","shell.execute_reply":"2025-01-01T20:51:32.091334Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59a98f07abb648ab821ad059a23639db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e71ab46c94f48fbabde78ff6eddc9ea"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Define model\nclass BertClassifier(nn.Module):\n    \"\"\"\n    BERT-based classifier for event classification.\n    \"\"\"\n    def __init__(self, num_classes):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n        self.dropout = nn.Dropout(0.3)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        dropout_output = self.dropout(pooled_output)\n        return self.classifier(dropout_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:32.092654Z","iopub.execute_input":"2025-01-01T20:51:32.092954Z","iopub.status.idle":"2025-01-01T20:51:32.097697Z","shell.execute_reply.started":"2025-01-01T20:51:32.092922Z","shell.execute_reply":"2025-01-01T20:51:32.096856Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Initialize model with filtered number of classes\nnum_classes = len(label_remap)  \nmodel = BertClassifier(num_classes)\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:32.099438Z","iopub.execute_input":"2025-01-01T20:51:32.099739Z","iopub.status.idle":"2025-01-01T20:51:35.932522Z","shell.execute_reply.started":"2025-01-01T20:51:32.099719Z","shell.execute_reply":"2025-01-01T20:51:35.931755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c65f6eeff97f46c8a917b85d234ab804"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Training and evaluation\ndef train_epoch(model, data_loader, criterion, optimizer):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    progress_bar = tqdm(data_loader, desc=f\"Training Epoch {epoch + 1}\", unit=\"batch\")\n        \n    for batch in progress_bar:\n        if batch is None: \n            continue\n            \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = torch.argmax(outputs, dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    return total_loss / len(data_loader), correct / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:35.933610Z","iopub.execute_input":"2025-01-01T20:51:35.934014Z","iopub.status.idle":"2025-01-01T20:51:35.939363Z","shell.execute_reply.started":"2025-01-01T20:51:35.933982Z","shell.execute_reply":"2025-01-01T20:51:35.938623Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def evaluate_epoch(model, data_loader, criterion):\n    \"\"\"\n    Evaluation function for one epoch.\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs, dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Generate classification report excluding \"No Event\"\n    filtered_target_names = [name for name in label_to_index.keys() if name != \"No Event\"]\n    report = classification_report(\n        all_labels, all_preds,\n        labels=[label_remap[v] for k, v in label_to_index.items() if k != \"No Event\"],\n        target_names=filtered_target_names\n    )\n    \n    return total_loss / len(data_loader), correct / total, report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:35.940255Z","iopub.execute_input":"2025-01-01T20:51:35.940574Z","iopub.status.idle":"2025-01-01T20:51:35.956881Z","shell.execute_reply.started":"2025-01-01T20:51:35.940547Z","shell.execute_reply":"2025-01-01T20:51:35.956285Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Training loop\nfor epoch in range(EPOCHS):\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n    val_loss, val_acc, report = evaluate_epoch(model, val_loader, criterion)\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n    print(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:51:35.957716Z","iopub.execute_input":"2025-01-01T20:51:35.957967Z","iopub.status.idle":"2025-01-01T20:58:39.004048Z","shell.execute_reply.started":"2025-01-01T20:51:35.957948Z","shell.execute_reply":"2025-01-01T20:58:39.003287Z"}},"outputs":[{"name":"stderr","text":"Training Epoch 1: 100%|██████████| 174/174 [00:38<00:00,  4.48batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 0.7248, Train Accuracy: 0.7879\nVal Loss: 0.3325, Val Accuracy: 0.9095\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.95      0.93      0.94       174\n    Gene_expression       0.88      0.94      0.91       335\n         Regulation       0.81      0.92      0.86       161\n      Transcription       0.91      0.70      0.79        90\nPositive_regulation       0.93      0.92      0.93       450\n            Binding       0.98      0.95      0.96       128\n       Localization       0.94      0.74      0.83        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.93      0.87      0.90        15\n\n           accuracy                           0.91      1426\n          macro avg       0.92      0.88      0.90      1426\n       weighted avg       0.91      0.91      0.91      1426\n\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 174/174 [00:38<00:00,  4.56batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10\nTrain Loss: 0.3026, Train Accuracy: 0.9163\nVal Loss: 0.3298, Val Accuracy: 0.9067\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.94      0.94      0.94       174\n    Gene_expression       0.87      0.95      0.91       335\n         Regulation       0.89      0.85      0.87       161\n      Transcription       0.98      0.63      0.77        90\nPositive_regulation       0.94      0.92      0.93       450\n            Binding       0.92      0.95      0.93       128\n       Localization       0.68      0.84      0.75        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.88      0.93      0.90        15\n\n           accuracy                           0.91      1426\n          macro avg       0.89      0.89      0.89      1426\n       weighted avg       0.91      0.91      0.91      1426\n\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 174/174 [00:39<00:00,  4.46batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10\nTrain Loss: 0.2660, Train Accuracy: 0.9205\nVal Loss: 0.3261, Val Accuracy: 0.9011\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.96      0.93      0.95       174\n    Gene_expression       0.91      0.92      0.91       335\n         Regulation       0.82      0.87      0.84       161\n      Transcription       0.91      0.70      0.79        90\nPositive_regulation       0.89      0.92      0.91       450\n            Binding       0.98      0.93      0.95       128\n       Localization       0.76      0.81      0.79        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.93      0.93      0.93        15\n\n           accuracy                           0.90      1426\n          macro avg       0.90      0.89      0.89      1426\n       weighted avg       0.90      0.90      0.90      1426\n\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4: 100%|██████████| 174/174 [00:38<00:00,  4.49batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10\nTrain Loss: 0.2542, Train Accuracy: 0.9235\nVal Loss: 0.3269, Val Accuracy: 0.9039\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.97      0.93      0.95       174\n    Gene_expression       0.89      0.93      0.91       335\n         Regulation       0.80      0.89      0.84       161\n      Transcription       0.88      0.71      0.79        90\nPositive_regulation       0.91      0.91      0.91       450\n            Binding       0.98      0.92      0.95       128\n       Localization       0.92      0.79      0.85        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.88      0.93      0.90        15\n\n           accuracy                           0.90      1426\n          macro avg       0.91      0.89      0.90      1426\n       weighted avg       0.91      0.90      0.90      1426\n\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5: 100%|██████████| 174/174 [00:38<00:00,  4.47batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10\nTrain Loss: 0.2382, Train Accuracy: 0.9289\nVal Loss: 0.3457, Val Accuracy: 0.9053\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.92      0.96      0.94       174\n    Gene_expression       0.89      0.93      0.91       335\n         Regulation       0.82      0.88      0.85       161\n      Transcription       0.90      0.71      0.80        90\nPositive_regulation       0.94      0.90      0.92       450\n            Binding       0.93      0.96      0.95       128\n       Localization       0.83      0.79      0.81        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.93      0.87      0.90        15\n\n           accuracy                           0.91      1426\n          macro avg       0.90      0.89      0.89      1426\n       weighted avg       0.91      0.91      0.90      1426\n\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6: 100%|██████████| 174/174 [00:39<00:00,  4.46batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10\nTrain Loss: 0.2258, Train Accuracy: 0.9282\nVal Loss: 0.3478, Val Accuracy: 0.9011\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.94      0.94      0.94       174\n    Gene_expression       0.88      0.94      0.91       335\n         Regulation       0.87      0.80      0.83       161\n      Transcription       0.90      0.70      0.79        90\nPositive_regulation       0.89      0.94      0.91       450\n            Binding       0.98      0.91      0.95       128\n       Localization       0.97      0.70      0.81        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.93      0.93      0.93        15\n\n           accuracy                           0.90      1426\n          macro avg       0.92      0.87      0.89      1426\n       weighted avg       0.90      0.90      0.90      1426\n\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7: 100%|██████████| 174/174 [00:38<00:00,  4.46batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10\nTrain Loss: 0.2240, Train Accuracy: 0.9300\nVal Loss: 0.3314, Val Accuracy: 0.9102\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.93      0.94      0.94       174\n    Gene_expression       0.88      0.95      0.92       335\n         Regulation       0.88      0.86      0.87       161\n      Transcription       0.86      0.71      0.78        90\nPositive_regulation       0.92      0.92      0.92       450\n            Binding       0.97      0.95      0.96       128\n       Localization       0.97      0.77      0.86        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.93      0.93      0.93        15\n\n           accuracy                           0.91      1426\n          macro avg       0.92      0.89      0.90      1426\n       weighted avg       0.91      0.91      0.91      1426\n\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8: 100%|██████████| 174/174 [00:38<00:00,  4.48batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10\nTrain Loss: 0.2189, Train Accuracy: 0.9297\nVal Loss: 0.3260, Val Accuracy: 0.9081\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.96      0.93      0.94       174\n    Gene_expression       0.89      0.94      0.92       335\n         Regulation       0.86      0.87      0.87       161\n      Transcription       0.86      0.71      0.78        90\nPositive_regulation       0.91      0.93      0.92       450\n            Binding       0.98      0.92      0.95       128\n       Localization       0.87      0.77      0.81        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.93      0.93      0.93        15\n\n           accuracy                           0.91      1426\n          macro avg       0.91      0.89      0.90      1426\n       weighted avg       0.91      0.91      0.91      1426\n\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9: 100%|██████████| 174/174 [00:39<00:00,  4.46batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10\nTrain Loss: 0.2126, Train Accuracy: 0.9297\nVal Loss: 0.3318, Val Accuracy: 0.9004\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.95      0.94      0.94       174\n    Gene_expression       0.90      0.92      0.91       335\n         Regulation       0.88      0.80      0.84       161\n      Transcription       0.86      0.71      0.78        90\nPositive_regulation       0.89      0.94      0.91       450\n            Binding       0.94      0.95      0.95       128\n       Localization       0.78      0.74      0.76        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.93      0.87      0.90        15\n\n           accuracy                           0.90      1426\n          macro avg       0.90      0.87      0.88      1426\n       weighted avg       0.90      0.90      0.90      1426\n\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10: 100%|██████████| 174/174 [00:38<00:00,  4.48batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10\nTrain Loss: 0.2094, Train Accuracy: 0.9349\nVal Loss: 0.3424, Val Accuracy: 0.9046\n                     precision    recall  f1-score   support\n\nNegative_regulation       0.94      0.94      0.94       174\n    Gene_expression       0.90      0.93      0.91       335\n         Regulation       0.88      0.80      0.83       161\n      Transcription       0.86      0.71      0.78        90\nPositive_regulation       0.89      0.95      0.92       450\n            Binding       0.96      0.97      0.96       128\n       Localization       0.94      0.74      0.83        43\n    Phosphorylation       0.94      1.00      0.97        30\n Protein_catabolism       0.93      0.87      0.90        15\n\n           accuracy                           0.90      1426\n          macro avg       0.92      0.88      0.89      1426\n       weighted avg       0.90      0.90      0.90      1426\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/model_checkpoint.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:58:39.004854Z","iopub.execute_input":"2025-01-01T20:58:39.005164Z","iopub.status.idle":"2025-01-01T20:58:39.603791Z","shell.execute_reply.started":"2025-01-01T20:58:39.005130Z","shell.execute_reply":"2025-01-01T20:58:39.603075Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:58:39.604628Z","iopub.execute_input":"2025-01-01T20:58:39.604837Z","iopub.status.idle":"2025-01-01T20:58:39.609251Z","shell.execute_reply.started":"2025-01-01T20:58:39.604819Z","shell.execute_reply":"2025-01-01T20:58:39.608495Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"download_file('/kaggle/working/model_checkpoint.pt', 'model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:58:39.610070Z","iopub.execute_input":"2025-01-01T20:58:39.610316Z","iopub.status.idle":"2025-01-01T20:59:02.022886Z","shell.execute_reply.started":"2025-01-01T20:58:39.610297Z","shell.execute_reply":"2025-01-01T20:59:02.022036Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/model.zip","text/html":"<a href='model.zip' target='_blank'>model.zip</a><br>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"#dmis-lab/biobert-base-cased-v1.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:59:02.023732Z","iopub.execute_input":"2025-01-01T20:59:02.023972Z","iopub.status.idle":"2025-01-01T20:59:02.027335Z","shell.execute_reply.started":"2025-01-01T20:59:02.023951Z","shell.execute_reply":"2025-01-01T20:59:02.026456Z"}},"outputs":[],"execution_count":15}]}